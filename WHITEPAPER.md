# üìÑ The MARLE Whitepaper: A Cognitive Prosthesis
**MindKrutch-AI Relational Logic Engine (MARLE v3.0)**

---

## I. Introduction: The Need for an Explainable Cognitive Prosthesis

### 1.1 The Problem
Traditional AI reasoning systems, such as large language models (LLMs), operate as "black boxes," making it difficult to trace the confidence level of a conclusion back to the originating evidence. In high-stakes decision environments (e.g., finance, medicine), a system that merely gives an answer is insufficient; one must know **why** and **how confident** that answer is. Furthermore, systems must adapt to the inherent uncertainty and temporal decay of real-world information.

### 1.2 The Solution: The MARLE Prototype
The **MindKrutch-AI Relational Logic Engine (MARLE)** is a prototype designed to solve this problem. It is a **graph-based logic engine** that emphasizes **explainability, dynamics, and adaptation**. Every conclusion generated by MARLE includes a **full provenance trace** detailing the contributing evidence and logical path.

### 1.3 Core Inspiration: Limited Quantum-Inspired Graph (LQG) Theory
MARLE is inspired by principles derived from **Loop Quantum Gravity (LQG)** and related discrete relational physics, specifically:
- **Discrete States (Quantization):** Information is not continuous but exists in well-defined states.
- **Relational Structure:** Truth is defined by the relationships (edges) between propositions (nodes), not by absolute external coordinates.
- **Dynamics:** The certainty of information is not static but evolves over time.

---

## II. MARLE Core Architecture (v2.0)
MARLE is built around a **flexible Knowledge Graph (KG)** where nodes represent propositions and edges represent logical relationships.

### 2.1 Nodes and Propositions
Nodes are categorized to provide semantic richness for explainability:
- **Observation:** Direct, measured facts (default certainty subject to decay).
- **Assumption:** Premises or priors introduced by the user or domain model (default certainty configurable).
- **Proposition:** General claims derived from other nodes.
- **CompositionNode:** Special nodes defining logical operations (AND, OR, NOT, IF-THEN).

### 2.2 Quantized States
Certainty is represented by a **scalar value** and quantized into four discrete, interpretable states. This prevents minor computational rounding errors from altering the perceived logical state.
- **Levels:** {0.0, 0.33, 0.67, 1.0}
- **Interpretation:** Impossible, Unlikely, Likely, Certain

### 2.3 Propagation and Convergence
The system uses an **iterative message-passing algorithm**. The certainty state of each node is computed from its inputs. This process repeats until the states of all nodes stabilize (**convergence**), ensuring that even graphs with cyclical dependencies reach a coherent, stable truth state.

---

## III. Structured Reasoning and Logic (v2.2)
MARLE implements sophisticated logical operators as special **Composition Nodes**, allowing for complex, structured reasoning chains.

### 3.1 Composition Operators
The composition rules are chosen for their desirable conservative and robust properties:

| Operator      | Symbol | Rule                                      | Rationale                                                                                     |
|---------------|--------|-------------------------------------------|-----------------------------------------------------------------------------------------------|
| Conjunction    | ‚àß      | PRODUCT(C_P √ó C_Q)                        | Conservative: Certainty is multiplied, rapidly decreasing overall confidence if any input is weak. |
| Disjunction    | ‚à®      | PROB_SUM(C_P + C_Q - (C_P √ó C_Q))         | Robust: Correctly aggregates parallel lines of evidence, preventing single weak supports from drowning strong ones. |
| Implication    | ‚äÉ      | min(1, 1 - C_P + C_Q)                     | Causal/Conditional: ≈Åukasiewicz Implication correctly models that P ‚äÉ Q is vacuously certain if P is impossible. |

### 3.2 Weighted Inputs (v2.1)
All inputs to a Composition Node are subject to an **adjustable edge weight (w)**, representing the relevance or trust in that specific relationship.
- The weighted value is then used in the composition calculation.
- This allows the system to model highly reliable evidence (high C_raw) that is deemed irrelevant (low w_edge), or vice versa.

---

## IV. Dynamic System Features (v2.4 - v2.5)
To model the real world, MARLE must account for the passage of time.

### 4.1 Temporal Dynamics: Evidence Decay (v2.3)
The certainty of **Observations** and **Assumptions** automatically degrades over time, preventing stale data from unduly influencing a decision. A **linear decay model** is used for computational simplicity and explainability:

**Formula:**
C_effective = C_raw √ó e^(-Œª √ó Œît)

Where:
- Œª = decay rate (e.g., 0.01 per day)
- Œît = age of the information in days

### 4.2 Artificial Heredity and Persistence (v2.4)
The system supports **saving and loading complete session snapshots** (persistence). When a snapshot is loaded and the system clock is advanced, all evidence automatically decays, leading to the "evolution" of belief states without recalculation. This forms the system's **long-term memory**.

### 4.3 Self-Awareness: Refresh Recommendations (v2.5)
The system actively scans its knowledge base and provides **actionable guidance**:
- **Mechanism:** Identifies Observations and Assumptions where C_effective has decayed below a threshold (e.g., 0.5) or whose age exceeds a domain-specific limit.
- **Output:** Generates recommendations with **Urgency Classification** (CRITICAL, HIGH, MEDIUM) and an **Impact Analysis** tracing the stale node to the final decision.

---

## V. Adaptive Intelligence (v3.0)
MARLE closes the cognitive loop by adapting its internal parameters based on observed outcomes, transforming it into a **self-improving prosthesis**.

### 5.1 The Learning Loop
The system tracks outcomes for key propositions and uses the error between the historical prediction (C_predicted) and the actual outcome (C_actual) to adjust its configuration.
- A **negative error** (overconfidence) triggers conservative adjustments.
- A **positive error** (underconfidence) triggers optimistic adjustments.

### 5.2 Adaptive Edge Weights
Edge weights (w) are adjusted using a **simple Delta Rule** approach proportional to the error and the node's historical contribution to the prediction. This tunes the system to trust reliable sources and paths more over time.

**Formula:**
Œîw = Œ± √ó (C_actual - C_predicted) √ó C_node

Where:
- Œ± = learning rate

### 5.3 Adaptive Decay Rates
The temporal decay rate (Œª) is also adjusted:
- If old, decaying evidence contributed to an **overconfident prediction**, Œª for that source type is **increased** (the data must age faster).
- If old, decaying evidence contributed to an **underconfident prediction**, Œª is **decreased** (the data is more enduring than assumed).
This ensures the system learns the true half-life of different types of evidence within its domain.

---

## VI. Conclusion and Future Work

### 6.1 Summary of Achievement
The **MindKrutch-AI Relational Logic Engine (MARLE v3.0)** is a complete, explainable, and adaptive prototype that successfully translates LQG-inspired principles into a practical AI system. It can:
- Model complex, structured causal dependencies.
- Provide a full, traceable provenance for every decision.
- Automatically track and adjust for the passage of time.
- Learn from past errors to refine its parameters autonomously.

### 6.2 Next Steps for Evolution
The path forward focuses on **accessibility, scaling, and advanced context modeling**:
- **Visualization:** Creating a time-series dashboard to show belief evolution.
- **Multi-Agent/Context Reasoning:** Allowing different user roles (contexts) to view different subgraphs or use different decay rates, enabling true multi-perspective analysis.
- **Natural Language Interface:** Parsing user questions directly into graph construction and query operations.

---
**END OF DOCUMENT**
